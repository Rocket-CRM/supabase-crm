# CRM Event-Driven Architecture

## Overview

The CRM system uses an event-driven architecture built on **Change Data Capture (CDC)**, **Kafka**, and **consumer microservices** for real-time processing of loyalty program events (currency awards, tier evaluations, missions, etc.).

---

## Architecture Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Supabase PostgreSQL                          ‚îÇ
‚îÇ  Tables: purchase_ledger, wallet_ledger, referral_ledger, etc.     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚îÇ (1) PostgreSQL Replication Slot
                         ‚îÇ     pg_replication_slots
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Confluent CDC Connector (crm-cdc-source)               ‚îÇ
‚îÇ  Type: PostgreSQL CDC Source V2 (Debezium)                          ‚îÇ
‚îÇ  - Monitors replication slot: crm_cdc_slot                          ‚îÇ
‚îÇ  - Reads from publication: crm_cdc_publication                      ‚îÇ
‚îÇ  - Snapshot mode: when_needed (recover from WAL loss)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚îÇ (2) Publishes Debezium events
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Confluent Kafka Cluster                         ‚îÇ
‚îÇ  Topics:                                                             ‚îÇ
‚îÇ  - crm.public.purchase_ledger                                       ‚îÇ
‚îÇ  - crm.public.wallet_ledger                                         ‚îÇ
‚îÇ  - crm.public.referral_ledger                                       ‚îÇ
‚îÇ  - crm.public.mission_claims                                        ‚îÇ
‚îÇ  - crm.public.codes                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚îÇ (3) Consumers subscribe to topics
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ        Render Background Worker: crm-event-processors               ‚îÇ
‚îÇ  Consumer Group: crm-event-processors                                ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  CurrencyConsumer (crm-event-processors-currency)           ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Subscribes: purchase_ledger                              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Concurrency: 5 parallel processes (eachBatch + p-limit)  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Deduplication: Redis (5-min window)                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Output: Publishes to Inngest for delayed awards          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  TierConsumer (crm-event-processors-tier)                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Subscribes: purchase_ledger, wallet_ledger               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Concurrency: 3 parallel processes (eachBatch + p-limit)  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Deduplication: Redis (5-min window)                      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Output: Direct tier upgrade or Inngest for delayed       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  MissionConsumer (crm-event-processors-mission)             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Subscribes: purchase_ledger, form_submissions            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Evaluates mission progress and completion                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Output: Publishes mission evaluation to Inngest          ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  (Additional consumers: RewardConsumer, MarketplaceConsumer)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚îÇ (4) Publishes events
                          ‚îÇ
                          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      Inngest (Workflow Engine)                       ‚îÇ
‚îÇ  Handles:                                                            ‚îÇ
‚îÇ  - Delayed currency awards (scheduled/rolling days)                 ‚îÇ
‚îÇ  - Delayed tier upgrades (start of month, etc.)                     ‚îÇ
‚îÇ  - Cancellation on refund (cancelOn events)                         ‚îÇ
‚îÇ  - Durable execution & retry logic                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## How CDC Connector Works (crm-cdc-source)

### 1. **PostgreSQL Replication Setup**

The CDC connector relies on PostgreSQL's logical replication:

```sql
-- Publication (defines which tables to track)
CREATE PUBLICATION crm_cdc_publication 
FOR TABLE 
  public.purchase_ledger, 
  public.wallet_ledger;

-- Replication Slot (tracks position in WAL log)
-- Created automatically by the connector
SELECT * FROM pg_replication_slots WHERE slot_name = 'crm_cdc_slot';
```

**Key Concepts:**
- **Publication**: Defines which tables/operations (INSERT/UPDATE/DELETE) to capture
- **Replication Slot**: Tracks the LSN (Log Sequence Number) position in the Write-Ahead Log (WAL)
- **WAL (Write-Ahead Log)**: PostgreSQL's transaction log - contains all database changes

### 2. **Debezium CDC Process**

The Confluent CDC connector is built on Debezium, which:

1. **Connects to PostgreSQL** via replication protocol
2. **Reads from replication slot** starting at the last committed LSN
3. **Parses WAL events** into structured change events
4. **Publishes to Kafka** with Debezium format:

```json
{
  "op": "c",  // c=create, u=update, d=delete
  "before": null,  // Previous row state (for updates/deletes)
  "after": {   // New row state
    "id": "123e4567-e89b-12d3-a456-426614174000",
    "user_id": "user_123",
    "merchant_id": "merchant_456",
    "final_amount": 100.00,
    "status": "completed",
    "earn_currency": true,
    "created_at": "2026-02-01T09:30:00Z"
  },
  "source": {
    "lsn": 123456789,  // Log Sequence Number
    "txId": 98765
  },
  "ts_ms": 1706777400000
}
```

### 3. **Configuration**

```json
{
  "name": "crm-cdc-source",
  "connector.class": "PostgresCdcSourceV2",
  "database.hostname": "db.wkevmsedchftztoolkmi.supabase.co",
  "database.port": "5432",
  "database.dbname": "postgres",
  "database.user": "postgres",
  "database.password": "********",
  
  "slot.name": "crm_cdc_slot",
  "publication.name": "crm_cdc_publication",
  "table.include.list": "public.purchase_ledger, public.wallet_ledger",
  
  "snapshot.mode": "when_needed",  // CRITICAL: Allows recovery from WAL loss
  "topic.prefix": "crm",           // Creates topics: crm.public.purchase_ledger
  
  "kafka.api.key": "HHYCUOC5MCWE5NP4",
  "kafka.api.secret": "********",
  "tasks.max": "1"
}
```

**Important Settings:**
- `snapshot.mode: when_needed` - If the LSN is no longer available (WAL purged), the connector takes a fresh snapshot
- `snapshot.mode: never` - Fails if LSN is lost (caused our original issue)

---

## Render Consumer (crm-event-processors)

### Architecture

The consumer service runs as a **Render Background Worker** (Node.js):

**Repository:** `Rocket-CRM/crm-event-processors`  
**Service:** `srv-d56v5pogjchc7399dfqg` on Render

### Key Features

#### 1. **Batch + Parallel Processing**

Using KafkaJS's `eachBatch` with `p-limit` for controlled concurrency:

```typescript
// CurrencyConsumer example
await this.consumer.run({
  partitionsConsumedConcurrently: 1,  // Process 1 partition at a time
  eachBatch: async ({ batch, resolveOffset, heartbeat, isRunning, isStale }) => {
    // Process messages in parallel with concurrency limit (5)
    const tasks = batch.messages.map((message) =>
      this.limit(async () => {
        // Parse Debezium message
        const debezium = parseDebeziumMessage(message.value);
        
        // Process currency award
        await this.processCurrencyAward(...);
        
        // Mark offset as processed
        resolveOffset(message.offset);
        await heartbeat();  // Prevent rebalancing
      })
    );
    
    await Promise.all(tasks);
  },
});
```

**Performance:**
- **Sequential (old):** 5-10 messages/sec
- **Batch + Parallel (new):** 30-40 messages/sec (**6-8x faster**)

#### 2. **Deduplication**

Redis-based deduplication (5-minute window) prevents duplicate processing:

```typescript
const dedupKey = currencyDedupKey('purchase', purchaseId);
const duplicate = await isDuplicate(dedupKey, 300); // 300 seconds

if (duplicate) {
  console.log('Duplicate detected, skipping');
  return;
}
```

#### 3. **Event Loop Keep-Alive**

Critical fix to prevent Node.js from exiting:

```typescript
// src/index.ts
async function main() {
  // Start all consumers
  await consumers.currency.start();
  await consumers.tier.start();
  // ...
  
  // CRITICAL: Keep process alive - consumers run indefinitely
  await new Promise(() => {}); // Never resolves
}
```

Without this, Node.js exits after `main()` completes, causing premature `SIGTERM`.

---

## The Rebalancing Issue & Root Cause

### Timeline of Events

#### **Phase 1: Initial Symptoms** (Jan 30-31)
- ‚ùå Render logs showed continuous `ERROR: The group is rebalancing`
- ‚ùå Consumers kept rejoining, no messages processed
- ‚ùå Service received repeated `SIGTERM` signals and restarted

#### **Phase 2: Initial Debugging** (Jan 31)
Attempted fixes:
1. ‚úÖ Added proper `heartbeat()` calls in consumer code
2. ‚úÖ Fixed Node.js keep-alive logic (`await new Promise(() => {})`)
3. ‚úÖ Changed consumer group ID to isolate testing
4. ‚ùå Rebalancing continued despite all fixes

#### **Phase 3: Discovery** (Feb 1)
- üîç Found Confluent CDC connector `crm-cdc-source` in **FAILED** state
- üîç Connector was crashing repeatedly with:
  - `ConnectException: Unable to obtain valid replication slot`
  - `DebeziumException: LSN no longer available`

#### **Phase 4: Root Cause Analysis** (Feb 1)

**The CDC connector was the culprit:**

1. **Stale Replication Slot**
   ```sql
   SELECT * FROM pg_replication_slots WHERE slot_name = 'crm_cdc_slot';
   -- Slot was "active: false" and "restart_lsn" was old
   ```

2. **LSN Expired**
   - Connector config had `snapshot.mode: never`
   - PostgreSQL had purged old WAL data (LSN no longer available)
   - Connector couldn't resume from its last offset

3. **Missing Kafka ACLs**
   - API key `HHYCUOC5MCWE5NP4` lacked topic-level permissions
   - Connector couldn't write to `crm.*` topics
   - Authorization errors caused repeated failures

**Why This Caused Rebalancing:**

When a Kafka connector crashes and restarts:
1. **Connector drops and recreates consumer connections**
2. **This triggers cluster-wide consumer group rebalancing**
3. **All consumers in the cluster (including crm-event-processors) must rejoin**
4. **Crashing every ~1 hour = continuous rebalancing**

### The Fix

#### **Step 1: Drop Stale Replication Slot**
```sql
SELECT pg_drop_replication_slot('crm_cdc_slot');
```

#### **Step 2: Delete Broken Connector**
```bash
# Via Confluent Cloud UI or API
DELETE /connectors/crm-cdc-source
```

#### **Step 3: Recreate Connector with Correct Config**
Key changes:
- ‚úÖ `snapshot.mode: when_needed` (instead of `never`)
- ‚úÖ Fresh start (no corrupted offset)
- ‚úÖ Proper slot/publication names

#### **Step 4: Add Kafka ACLs**
In Confluent Cloud ‚Üí API Keys ‚Üí `HHYCUOC5MCWE5NP4`:
- ‚úÖ Add `READ` permission for `crm.*` topics
- ‚úÖ Add `WRITE` permission for `crm.*` topics
- ‚úÖ Add `CREATE` permission for `crm.*` topics

#### **Step 5: Verify Connector Health**
```bash
# Connector status: RUNNING
# Task status: RUNNING
# No errors in logs
```

---

## Results After Fix

### Connector Health ‚úÖ
- **Status:** RUNNING
- **Tasks:** 1/1 running
- **Errors:** None
- **Messages flowing:** Yes (confirmed via consumer logs)

### Consumer Performance ‚úÖ
- **Rebalancing:** None (0 errors in 30+ minutes)
- **Throughput:** 30-40 messages/sec (6-8x improvement)
- **Stability:** No `SIGTERM` or premature exits
- **Backlog:** Catching up on ~10 hours of missed events

### System Architecture ‚úÖ
- **CDC ‚Üí Kafka:** Working
- **Kafka ‚Üí Consumers:** Working
- **Consumers ‚Üí Inngest:** Working
- **End-to-end latency:** <2 seconds (real-time)

---

## Key Lessons Learned

### 1. **CDC Connector is Critical Infrastructure**
- A failing CDC connector impacts the entire event-driven system
- Monitor connector health separately from consumer health
- Connector crashes cause cluster-wide rebalancing

### 2. **Snapshot Mode Matters**
- `snapshot.mode: never` is fragile - fails on WAL purge
- `snapshot.mode: when_needed` is resilient - recovers automatically
- Always plan for WAL retention expiration

### 3. **Kafka ACLs Must Match Service Accounts**
- Topic-level ACLs are required even if organization-level roles exist
- Missing permissions cause silent authorization failures
- Always verify ACLs after connector creation

### 4. **Debugging Kafka Issues**
- Check connector health FIRST before debugging consumer code
- Use Confluent Cloud UI to inspect connector status and logs
- Rebalancing can be caused by upstream issues, not just consumer code

### 5. **Node.js Event Loop for Long-Running Processes**
- Must explicitly keep event loop alive: `await new Promise(() => {})`
- Without this, Node.js exits after `main()` completes
- Causes mysterious `SIGTERM` signals on Render

---

## Monitoring & Observability

### Key Metrics to Monitor

#### **CDC Connector**
- Connector status (RUNNING/FAILED)
- Task status (RUNNING/FAILED)
- Connector lag (time behind database)
- Replication slot active status

#### **Kafka Topics**
- Message throughput (messages/sec)
- Topic lag (unconsumed messages)
- Consumer group lag per partition

#### **Consumers (Render)**
- Processing throughput (messages/sec)
- Rebalancing frequency (should be 0)
- Error rate
- Deduplication hit rate

#### **PostgreSQL**
- Replication slot lag: `pg_replication_slots.restart_lsn`
- WAL disk usage: `pg_current_wal_lsn()` vs `restart_lsn`
- Active replication connections

### Alerting Rules

```yaml
alerts:
  - name: CDC Connector Down
    condition: connector.status != RUNNING
    severity: critical
    
  - name: Consumer Lag High
    condition: consumer.lag > 1000 messages
    severity: warning
    
  - name: Replication Slot Inactive
    condition: pg_replication_slots.active = false
    severity: critical
    
  - name: Consumer Rebalancing
    condition: consumer.rebalances > 0 in last 5 minutes
    severity: warning
```

---

## Future Improvements

### 1. **RisingWave Integration** (Planned)
- Use Kafka topics as source for real-time materialized views
- Provide AI agents with always-up-to-date context
- Enable real-time analytics dashboards

### 2. **Dead Letter Queue**
- Handle permanently failed messages
- Manual retry/inspection workflow

### 3. **Consumer Autoscaling**
- Scale consumers based on Kafka lag
- Render supports horizontal scaling

### 4. **Enhanced Observability**
- Datadog/Grafana dashboards
- Distributed tracing (OpenTelemetry)
- Kafka lag metrics in Prometheus

---

## Quick Reference

### Useful Commands

```sql
-- Check replication slots
SELECT * FROM pg_replication_slots;

-- Check WAL position
SELECT pg_current_wal_lsn();

-- Drop replication slot (if needed)
SELECT pg_drop_replication_slot('crm_cdc_slot');

-- Check publication
SELECT * FROM pg_publication WHERE pubname = 'crm_cdc_publication';
```

### Environment Variables (Render)

```bash
KAFKA_BOOTSTRAP_SERVERS=pkc-*.confluent.cloud:9092
KAFKA_API_KEY=***
KAFKA_API_SECRET=***

SUPABASE_URL=https://wkevmsedchftztoolkmi.supabase.co
SUPABASE_SERVICE_ROLE_KEY=***

REDIS_URL=rediss://***
CRM_CACHE_REDIS_URL=rediss://***

INNGEST_EVENT_KEY=***

CONSUMER_GROUP_ID=crm-event-processors
CURRENCY_CONCURRENCY=5
TIER_CONCURRENCY=3
```

### Useful Links

- **Render Service:** https://dashboard.render.com/worker/srv-d56v5pogjchc7399dfqg
- **Confluent Cloud:** https://confluent.cloud
- **GitHub Repo:** https://github.com/Rocket-CRM/crm-event-processors
- **Supabase Dashboard:** https://supabase.com/dashboard/project/wkevmsedchftztoolkmi

---

## Support & Troubleshooting

### Common Issues

#### Issue: "The group is rebalancing"
**Symptoms:** Continuous rebalancing errors in logs  
**Root Cause:** CDC connector crashing, network issues, or heartbeat timeouts  
**Fix:** Check CDC connector health first, then consumer heartbeat() calls

#### Issue: "Unable to obtain valid replication slot"
**Symptoms:** CDC connector fails on startup  
**Root Cause:** Stale or broken replication slot in PostgreSQL  
**Fix:** Drop and recreate the slot via connector recreation

#### Issue: "LSN no longer available"
**Symptoms:** CDC connector can't resume from last offset  
**Root Cause:** WAL retention expired, `snapshot.mode: never`  
**Fix:** Recreate connector with `snapshot.mode: when_needed`

#### Issue: Node.js process exits with SIGTERM
**Symptoms:** Render service exits every few minutes  
**Root Cause:** Missing event loop keep-alive (`await new Promise(() => {})`)  
**Fix:** Add infinite promise to keep event loop alive

---

**Last Updated:** Feb 1, 2026  
**Version:** 2.0 (Batch + Parallel Processing)
